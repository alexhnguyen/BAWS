# BERT Assisted Word Substitution (BAWS)

## Introduction

Bidirectional Encoder Representations from Transformers [(BERT)](https://github.com/google-research/bert) is a language 
model from Google, which uses a bidirectional transformer pre-trained on a large corpus using significant computing 
power. BERT has been shown to be exceed prior models on several benchmarks including question answering and next 
sentence prediction. We explore the potential of BERT to enhance existing lossless compression techniques.

## Information

A [google colab](https://colab.research.google.com/drive/13HVJZ1jYAFJ9KdsFoiM8yKVC5aUmsevL) of [baws.ipynb](baws.ipynb)
is available. 



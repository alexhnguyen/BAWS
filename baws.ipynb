{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMORgvTleU3z",
    "colab_type": "text"
   },
   "source": [
    "MOTIVATION\n",
    "\n",
    "Lossless compression reduces the stored size of data by transforming to an alternate representation. In lossless compression schemes the input can be reconstructed perfectly, unlike lossy methods which may lose some data which is considered 'less important'.\n",
    "\n",
    "Typical compression utilities include GZip and XZ, which are both sliding window dictionary based methods. These algorithms exploit repetition in text, but do not consider the content of the text itself. Common phrases and patterns which exist in natural language however are not explicitly leveraged. In this sense some information lays outside of the data itself, embedded in the language.\n",
    "\n",
    "Bidirectional Encoder Representations from Transformers (BERT) is a language model from Google, which uses a bidirectional transformer pre-trained on a large corpus using significant computing power. BERT has been shown to be exceed prior models on several benchmarks including question answering and next sentence prediction. We explore the potential of BERT to enhance existing lossless compression techniques.\n",
    "\n",
    "---\n",
    "APPROACH\n",
    "\n",
    "Given an input sentence, we aim to substitute a subset of the sentence with placeholders to improve the efficacy of subsequent compression. In order to achieve lossless compression, we ensure that the operation is invertible before performing the replacement.\n",
    "\n",
    "The BERT model is used for both compression and decompression tasks. The input text is is first tokenized in two phases to preserve mappings from token to input text. The basic tokenization is easily mapped back to the original text, while the WordPiece tokenization is required for pre-trained BERT. Compression involves determining a combination of tokens which can be masked and predicted with BERT to recover the original sentence. Any such sentence is considered to be a valid substitution. We configure BERT to use one previous sentence along with the current sentence for context. Since decompression is performed on sentence in order, the previous sentence is always available in uncompressed form.\n",
    "\n",
    "Checking the invertibility of substitution is computationally intensive, thus it is important to minimize use of this operation. At the same time, it is desirable have more of the sentence masked to achieve higher compression. Note that some maskings will produce higher compression than others even if the length or count of the words is identical. This is a property of the subsequent coding methods, and will differ based on the methods used. \n",
    "\n",
    "Although it is possible to check all combinations of mask configurations for a given sentence, this will result in excessive runtime and is therefore undesirable. We therefore propose two heuristics to approximate a best solution.\n",
    "The first heuristic tries to remove words by length. We found preferring to remove short words over long words has better performance. It tends to remove words such as “the” and “to”.\n",
    "The second heuristic tries to remove words by impact. Impact is a measure we invented to describe the relationship a word has on other words. The impact, of a word w is how how many more or less words can be guessed when word w is removed. We prefer to remove words with high impact, or where the set of words that can be guessed afterwards is large. Unfortunately getting the impact of each word is costly, and the impact of all the words must be recomputed when a word is removed.\n",
    "\n",
    "Once a combination of mask positions is selected, word substitution is performed on the original text. Each mask is replaced with a placeholder symbol which represents a word to be masked when performing decompression. The substitute text is then compressed using well known coding methods to achieve the final compressed output. We use the Huffman coding algorithm as well as the GZip and XZ utilities to verify improvement of final compression ratios, however this technique should extend to other frequency based algorithms.\n",
    "\n",
    "---\n",
    "DATA\n",
    "\n",
    "We download randomized pages from Wikipedia and concatenate them together to form 5 datasets of 10,000 words each. We use the freely available Wikipedia API to perform the queries. We select 10,000 words in order to limit the runtime of compression. We compressed both large and small data using Huffman, Gzip, and XZ without BAWS to confirm that compression ratios are relatively stable down to our selected dataset size of 10,000 words.\n",
    "\n",
    "```\n",
    "pip install wikipedia\n",
    "```\n",
    "\n",
    "---\n",
    "CODE\n",
    "\n",
    "- Huffman coding, we used someone else's implementation, and change slightly\n",
    "    - https://github.com/bhrigu123/huffman-coding.\n",
    "- XZ and GZip easily installed on Ubuntu \n",
    "- use prebuilt BERT model\n",
    "   - https://github.com/huggingface/pytorch-pretrained-BERT\n",
    "   - wrote our own interface to use\n",
    "       - `MaskEvaluator` and `ReversibleBertTokenizer` classes\n",
    "- deciding what words to mask is “difficult”\n",
    "   - wrote our own code to choose what words to mask\n",
    "       - get_by_length and get_by_impact functions\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "sudo apt-get install gzip\n",
    "sudo apt-get install xz\n",
    "pip install pytorch-pretrained-bert\n",
    "```\n",
    "\n",
    "---\n",
    "EXPERIMENTAL SETUP\n",
    "\n",
    "We run standard compression without substitution on the 5 datasets, averaging the results to obtain a baseline compressed size. We then repeat the process for the substituted text (BAWS) and compare the final compressed size to the baseline compressed size.\n",
    "\n",
    "---\n",
    "RESULTS\n",
    "\n",
    "To the best of our knowledge, prior work on language model based word compression is limited. One comparable paper https://nlp.stanford.edu/courses/cs224n/2006/fp/aeldaher-jconnor-1-report.pdf achieves compression of approximately 30%, however we we unable to obtain the dataset for a direct comparison. Small datasets were also used in this paper due to long runtimes.\n",
    "\n",
    "We found that using BAWS results in approximately 25% of words in a sample text to be replaced however this does not measure the actual efficacy in a compression context. To ensure that entropy is reduced with respect to common algorithms, we measure the final compressed size both with and without BAWS as a percent difference between compressed, and compressed with BAWS.\n",
    "\n",
    "$\\frac{\\text{basic_compression_size - baws_compression_size}}{\\text{basic_compression_size}}$\n",
    "\n",
    "Huffman coding benefits most from BAWS. We found BAWS to reduce the final compressed size by an additional 32.54%. The GZip and XZ utilities used in conjunction with BAWS results in 24.03% and 24.61% difference respectively.\n",
    "\n",
    "\n",
    "\t\n",
    "---\n",
    "\n",
    "\n",
    "ANALYSIS OF THE RESULTS\n",
    "\n",
    "The difference between the character and sequence based compression methods seems to indicate that the character entropy is reduced more than the word entropy. Further investigation is required to identify the source of these observed differences.\n",
    "\n",
    "The runtime to compress 10,000 words (8th Gen Intel I7 laptop) is approximately 30 minutes. This runtime prevented evaluation of larger files.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "FUTURE WORK\n",
    "\n",
    "One significant issue with BAWS is the long runtimes associated with selecting words to mask. The checking of invertibility is computationally expensive since the model must be evaluated. By identifying valid combinations with less attempts, a significant speedup should be observed. It is likely that this problem can be modeled probabilistically, replacing the proposed “get by length” and “get by impact” heuristics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "DP601xz7MZ3D",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pytorch-pretrained-bert\n",
    "!pip install numpy\n",
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vUENcc32MVgr",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 64.0
    },
    "outputId": "73eab7d8-6574-427d-f1f6-8c3494cc549b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import wikipedia\n",
    "import time\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import heapq\n",
    "import os\n",
    "from functools import total_ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zE8bMOTdCSVo",
    "colab_type": "text"
   },
   "source": [
    "This class **MaskEvaluator**  manages the BERT model. The main functionality of the class is to \n",
    "1.   Tokenize sentences\n",
    "2.   Determine if a group of indexes can be removed from a sentence.\n",
    "\n",
    "`me.evaluate` returns an array where the first index is `True` if the group of indexes can be removed from the sentence. The second index is an array that returns what indexes should be removed to return `True` (if already true an empty array is returned).\n",
    "\n",
    "EXAMPLE\n",
    "```\n",
    "text = 'i enjoy doing NLP. hopefully my group will do well on the project.'\n",
    "\n",
    "# the init will tokenize the sentence\n",
    "me = MaskEvaluator(text)\n",
    "\n",
    "print('')\n",
    "sentence_num = 1\n",
    "indexes_to_mask = [0, 2, 5]\n",
    "np_sentence = np.array(me.sentences[sentence_num])\n",
    "np_indexes = np.array(indexes_to_mask)\n",
    "sentence = ' '.join(me.sentences[sentence_num])\n",
    "words = np.array(me.sentences[1])[np.array(indexes_to_mask)]\n",
    "\n",
    "# calling me.evaluate will determine if a group of words can be removed\n",
    "if me.evaluate(sentence_num, indexes_to_mask)[0]:\n",
    "    print('From [{sentence}], the words {words} can be masked'\n",
    "          .format(sentence=sentence, words=words))\n",
    "else:\n",
    "    print('From [{sentence}], the words {words} cannot be masked'\n",
    "          .format(sentence=sentence, words=words))\n",
    "```\n",
    "--> * `From [hopefully my group will do well on the project .], the words ['hopefully' 'group' 'well'] cannot be masked` *\n",
    "\n",
    "**Choosing which group of words to remove is non-trivial.** For example, in the example the indexes \n",
    "`[0, 2, 5]` were chosen arbitrarily. Given infinite time, we would like to try all cases and pick the\n",
    "largest group of indexes. That is, if we could pick `[0, 5]` or `[0, 1, 3]`, we would pick the\n",
    "second one because it's length (3) is bigger.\n",
    "\n",
    "This problem will be explored later on in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "4J5fbg8MBsOj",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class ReversibleBertTokenizer(BertTokenizer):\n",
    "    '''\n",
    "    Extend the BertTokenizer to provide reversibilty\n",
    "    '''\n",
    "    def reversible_tokenize(self, text):\n",
    "        basic_tokens = self.basic_tokenizer.tokenize(text)\n",
    "        split_tokens = []\n",
    "        basic_index_map = {}\n",
    "        split_index_map = defaultdict(list)\n",
    "        for index, token in enumerate(basic_tokens):\n",
    "            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
    "                # Map the basic token index to the split token index\n",
    "                basic_index_map[len(split_tokens)] = index\n",
    "                # Map split tokens to a basic token\n",
    "                split_index_map[index].append(len(split_tokens))\n",
    "                # Append the subtoken\n",
    "                split_tokens.append(sub_token)\n",
    "\n",
    "        return split_tokens, basic_tokens, split_index_map, basic_index_map\n",
    "\n",
    "\n",
    "'''\n",
    "Evaluate a sentence masking configuration\n",
    "'''\n",
    "class MaskEvaluator:\n",
    "    def __init__(self, text):\n",
    "        # Load pre-trained model tokenizer (vocabulary)\n",
    "        self.tokenizer = ReversibleBertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "        self.model = BertForMaskedLM.from_pretrained('bert-base-cased')\n",
    "\n",
    "        # Tokenize\n",
    "        self.split_tokens, self.basic_tokens, self.split_index_map, self.basic_index_map = self.tokenizer.reversible_tokenize(text)\n",
    "\n",
    "        # Get sentence start indices\n",
    "        self.sentence_indices = [0]\n",
    "        for index, basic_token in enumerate(self.basic_tokens):\n",
    "            # TODO: Make these splits better\n",
    "            if basic_token == '.':\n",
    "                self.sentence_indices.append(index+1) # Point at the start of the next sentence\n",
    "\n",
    "        # TODO: This can be removed eventually. Currently will break application\n",
    "        self.sentences = [] # Only used by appliation code. Not required\n",
    "        for i in range(len(self.sentence_indices)-1):\n",
    "            self.sentences.append(self.get_sentence(i))\n",
    "\n",
    "    def get_count(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def get_sentence(self, index):\n",
    "        return deepcopy(self.basic_tokens[self.sentence_indices[index]:self.sentence_indices[index+1]])\n",
    "\n",
    "    def get_sentence_offset(self, index):\n",
    "        return self.sentence_indices[index]\n",
    "\n",
    "    def get_split_sentence(self, index):\n",
    "        return self.get_masked_split_sentence(index)\n",
    "\n",
    "    def get_masked_split_sentence(self, index, mask=None):\n",
    "        if mask is None:\n",
    "            mask = []\n",
    "        basic_offset = self.get_sentence_offset(index)\n",
    "        split_sentence = []\n",
    "        for basic_index, basic_token in enumerate(self.get_sentence(index)):\n",
    "            # Mask out all split tokens if basic token is masked\n",
    "            if basic_index in mask:\n",
    "                split_sentence.extend(['[MASK]' for _ in \n",
    "                    range(len(self.split_index_map[basic_offset+basic_index]))])\n",
    "            else:\n",
    "                split_sentence += deepcopy(\n",
    "                        [self.split_tokens[i] for i in \n",
    "                            self.split_index_map[basic_offset+basic_index]])\n",
    "        return split_sentence\n",
    "\n",
    "    '''\n",
    "    Evaluate with previous sentence context.\n",
    "    index -> target sentence index.\n",
    "    mask -> a list of indicies at which a mask should be placed. This requests\n",
    "        word replacement at this index. Mask indicies that do not exist are ignored.\n",
    "    '''\n",
    "    def evaluate(self, index, mask):\n",
    "        previous = [] if index == 0 else self.get_split_sentence(index-1)\n",
    "        current = self.get_split_sentence(index)\n",
    "        masked = self.get_masked_split_sentence(index, mask)\n",
    "\n",
    "        # Convert indicies\n",
    "        indexed = self.tokenizer.convert_tokens_to_ids(previous + masked)\n",
    "\n",
    "        # Define seperator mask for sentence A and B\n",
    "        segment_a = [0 for _ in range(len(previous))]\n",
    "        segment_b = [1 for _ in range(len(masked))]\n",
    "\n",
    "        # Convert inputs to tensors\n",
    "        tokens = torch.tensor([indexed])\n",
    "        segments = torch.tensor([segment_a + segment_b])\n",
    "\n",
    "        # Set model to evaluation mode\n",
    "        self.model.eval()\n",
    "\n",
    "        # Make predictions\n",
    "        predicted = self.model(tokens, segments)\n",
    "        predicted_indices = [torch.argmax(\n",
    "                predicted[0, i+len(previous)]).item() for i in range(len(masked))]\n",
    "        predicted_tokens = self.tokenizer.convert_ids_to_tokens(predicted_indices)\n",
    "\n",
    "        # Validate predictions\n",
    "        invalid = []\n",
    "        for i in mask:\n",
    "            if predicted_tokens[i] != current[i]:\n",
    "                invalid.append(i)\n",
    "        \n",
    "        return bool(len(invalid) == 0), invalid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV3hyuU8K0E6",
    "colab_type": "text"
   },
   "source": [
    "The class **HuffmanCoding** does Huffman coding. The code has been heavily borrowed from \n",
    "**https://github.com/bhrigu123/huffman-coding**. The code has been edited to fit our task. \n",
    "\n",
    "Huffman coding works by assigning variable length codes to characters. Characters that frequently\n",
    "appear will be assigned short length codes.\n",
    "\n",
    "For instance, suppose we had the word `hello`. We would assign the shortest possible length code to `l` because it is the most frequent character.\n",
    "\n",
    "EXAMPLE\n",
    "```\n",
    "hc = HuffmanCoding('')\n",
    "text = 'i enjoy doing NLP. hopefully my group will do well on the project.'\n",
    "hc.commpress_text(text, 'compressed_text.bin')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "h1KLYs3dMebA",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code for Huffman Coding, compression and decompression. \n",
    "Explanation at http://bhrigu.me/blog/2017/01/17/huffman-coding-python-implementation/\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@total_ordering\n",
    "class HeapNode:\n",
    "    def __init__(self, char, freq):\n",
    "        self.char = char\n",
    "        self.freq = freq\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    # defining comparators less_than and equals\n",
    "    def __lt__(self, other):\n",
    "        return self.freq < other.freq\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if (other == None):\n",
    "            return False\n",
    "        if (not isinstance(other, HeapNode)):\n",
    "            return False\n",
    "        return self.freq == other.freq\n",
    "\n",
    "\n",
    "class HuffmanCoding:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.heap = []\n",
    "        self.codes = {}\n",
    "        self.reverse_mapping = {}\n",
    "\n",
    "    # functions for compression:\n",
    "\n",
    "    def make_frequency_dict(self, text):\n",
    "        frequency = {}\n",
    "        for character in text:\n",
    "            if not character in frequency:\n",
    "                frequency[character] = 0\n",
    "            frequency[character] += 1\n",
    "        return frequency\n",
    "\n",
    "    def make_heap(self, frequency):\n",
    "        for key in frequency:\n",
    "            node = HeapNode(key, frequency[key])\n",
    "            heapq.heappush(self.heap, node)\n",
    "\n",
    "    def merge_nodes(self):\n",
    "        while (len(self.heap) > 1):\n",
    "            node1 = heapq.heappop(self.heap)\n",
    "            node2 = heapq.heappop(self.heap)\n",
    "\n",
    "            merged = HeapNode(None, node1.freq + node2.freq)\n",
    "            merged.left = node1\n",
    "            merged.right = node2\n",
    "\n",
    "            heapq.heappush(self.heap, merged)\n",
    "\n",
    "    def make_codes_helper(self, root, current_code):\n",
    "        if (root == None):\n",
    "            return\n",
    "\n",
    "        if (root.char != None):\n",
    "            self.codes[root.char] = current_code\n",
    "            self.reverse_mapping[current_code] = root.char\n",
    "            return\n",
    "\n",
    "        self.make_codes_helper(root.left, current_code + \"0\")\n",
    "        self.make_codes_helper(root.right, current_code + \"1\")\n",
    "\n",
    "    def make_codes(self):\n",
    "        root = heapq.heappop(self.heap)\n",
    "        current_code = \"\"\n",
    "        self.make_codes_helper(root, current_code)\n",
    "\n",
    "    def get_encoded_text(self, text):\n",
    "        encoded_text = \"\"\n",
    "        for character in text:\n",
    "            encoded_text += self.codes[character]\n",
    "        return encoded_text\n",
    "\n",
    "    def pad_encoded_text(self, encoded_text):\n",
    "        extra_padding = 8 - len(encoded_text) % 8\n",
    "        for i in range(extra_padding):\n",
    "            encoded_text += \"0\"\n",
    "\n",
    "        padded_info = \"{0:08b}\".format(extra_padding)\n",
    "        encoded_text = padded_info + encoded_text\n",
    "        return encoded_text\n",
    "\n",
    "    def get_byte_array(self, padded_encoded_text):\n",
    "        if (len(padded_encoded_text) % 8 != 0):\n",
    "            print(\"Encoded text not padded properly\")\n",
    "            exit(0)\n",
    "\n",
    "        b = bytearray()\n",
    "        for i in range(0, len(padded_encoded_text), 8):\n",
    "            byte = padded_encoded_text[i:i + 8]\n",
    "            b.append(int(byte, 2))\n",
    "        return b\n",
    "\n",
    "    def compress_text(self, text, output_path):\n",
    "        text = ''.join(text)\n",
    "        frequency = self.make_frequency_dict(text)\n",
    "        self.make_heap(frequency)\n",
    "        self.merge_nodes()\n",
    "        self.make_codes()\n",
    "        encoded_text = self.get_encoded_text(text)\n",
    "        padded_encoded_text = self.pad_encoded_text(encoded_text)\n",
    "        b = self.get_byte_array(padded_encoded_text)\n",
    "        with open(output_path, 'wb') as output:\n",
    "            output.write(bytes(b))\n",
    "        # return padded_encoded_text\n",
    "\n",
    "    def compress_file(self):\n",
    "        filename, file_extension = os.path.splitext(self.path)\n",
    "        output_path = filename + \".bin\"\n",
    "\n",
    "        with open(self.path, 'r+') as file, open(output_path, 'wb') as output:\n",
    "            text = file.read()\n",
    "            text = text.rstrip()\n",
    "\n",
    "            frequency = self.make_frequency_dict(text)\n",
    "            self.make_heap(frequency)\n",
    "            self.merge_nodes()\n",
    "            self.make_codes()\n",
    "\n",
    "            encoded_text = self.get_encoded_text(text)\n",
    "            padded_encoded_text = self.pad_encoded_text(encoded_text)\n",
    "\n",
    "            b = self.get_byte_array(padded_encoded_text)\n",
    "            output.write(bytes(b))\n",
    "\n",
    "        print(\"Compressed\")\n",
    "        return output_path\n",
    "\n",
    "    \"\"\" functions for decompression: \"\"\"\n",
    "\n",
    "    def remove_padding(self, padded_encoded_text):\n",
    "        padded_info = padded_encoded_text[:8]\n",
    "        extra_padding = int(padded_info, 2)\n",
    "\n",
    "        padded_encoded_text = padded_encoded_text[8:]\n",
    "        encoded_text = padded_encoded_text[:-1 * extra_padding]\n",
    "\n",
    "        return encoded_text\n",
    "\n",
    "    def decode_text(self, encoded_text):\n",
    "        current_code = \"\"\n",
    "        decoded_text = \"\"\n",
    "\n",
    "        for bit in encoded_text:\n",
    "            current_code += bit\n",
    "            if (current_code in self.reverse_mapping):\n",
    "                character = self.reverse_mapping[current_code]\n",
    "                decoded_text += character\n",
    "                current_code = \"\"\n",
    "\n",
    "        return decoded_text\n",
    "\n",
    "    def decompress(self, input_path):\n",
    "        filename, file_extension = os.path.splitext(self.path)\n",
    "        output_path = filename + \"_decompressed\" + \".txt\"\n",
    "\n",
    "        with open(input_path, 'rb') as file, open(output_path, 'w') as output:\n",
    "            bit_string = \"\"\n",
    "\n",
    "            byte = file.read(1)\n",
    "            while (len(byte) > 0):\n",
    "                byte = ord(byte)\n",
    "                bits = bin(byte)[2:].rjust(8, '0')\n",
    "                bit_string += bits\n",
    "                byte = file.read(1)\n",
    "\n",
    "            encoded_text = self.remove_padding(bit_string)\n",
    "\n",
    "            decompressed_text = self.decode_text(encoded_text)\n",
    "\n",
    "            output.write(decompressed_text)\n",
    "\n",
    "        print(\"Decompressed\")\n",
    "        return output_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dL9MS2ScQZH2",
    "colab_type": "text"
   },
   "source": [
    "Recall **choosing which group of words to remove is non-trivial.** The problem is sentences of length N has\n",
    "\n",
    "\\begin{equation*}\n",
    "\\left( \\sum_{k=1}^N \\binom{N}{k} \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "possible removable groups.\n",
    "\n",
    "Here we will explore different algorithms that will determine which group of words to choose. All of them will attempt to maximize the length of the group being removed. There are **definitely** better methods to determine the best group to remove. All of the algorithms listed follow a *greedy* approach. \n",
    "We only add indexes to the group so that using `me.evaluate` on the group returns `True` (recall \n",
    "this means we can still retrieve the original sentence). \n",
    "\n",
    "This approach is flawed. Consider \n",
    "```\n",
    "group1 = [0]\n",
    "group2 = [1]\n",
    "group3 = [0, 1]\n",
    "```\n",
    "It is possible `me.evaluate` returns `False` for `group1` and `group2`, but returns `True` for `group3`. Our algorithm would never get to `group3` in this case. However, we found our current methods are \"good enough\" by helping decompress by an additional ~25%.\n",
    "\n",
    "---\n",
    "\n",
    "Algorithm 1: **get_by_length** This algorithm sorts the words by length, and tries to remove indicies from the list in that order. Experiments show removing shortest or longest words first does not significantly change the number of words removed.\n",
    "\n",
    "---\n",
    "\n",
    "Algorithm 2: **get_by_impact** This algorithm sorts the words by impact, and tries to remove indicies from the list in that order. Impact is a definition we invented. The impact if an index given a sentence and indexes, is how much an index affects the ability of the BERT algorithm to retrieve the original sentence.\n",
    "\n",
    "For example, given a sentence = [w1, w2, w3, s4], and indexes = [2, 3, 4], we would like to measure the impact of the indexes. Recall the second value returned from `me.evaluate` is which indexes should be removed from the original indexes, for `me.evaluate` to return `True`\n",
    "*   impact of 2 is has value `len(me.evaluate(sentence, [3, 4])[1])`\n",
    "*   impact of 3 is has value `len(me.evaluate(sentence, [2, 4])[1])`\n",
    "*   impact of 4 is has value `len(me.evaluate(sentence, [2, 3])[1])`\n",
    "\n",
    "We can sort be largest impact or smallest impact. Our experiments show **sorting by largest impact outperforms the other algorithms on amount compressed.** However, **sorting by shortest word is much faster and performs similarily to largest impact**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "quDmBknqOKJG",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def get_single_index(me, s, sentence):\n",
    "    # see which words can be removed by itself\n",
    "    sentence_order = []\n",
    "    for c, word in enumerate(sentence[-1:]):\n",
    "        value = me.evaluate(s, [c])\n",
    "        if value[0]:\n",
    "            sentence_order.append(c)\n",
    "    return sentence_order\n",
    "\n",
    "\n",
    "def get_by_length(me, sort_by='largest'):\n",
    "    if sort_by not in ['largest', 'smallest']:\n",
    "        raise ValueError(\"[sort_by] must be largest or smallest\")\n",
    "    removable = []\n",
    "    sent_length = len(me.sentences)\n",
    "    for s, sentence in enumerate(me.sentences):\n",
    "        print(s/sent_length)\n",
    "        word_counter_ = []\n",
    "        \n",
    "        # sort by length of each word\n",
    "        for i, word in enumerate(sentence):\n",
    "            word_counter_.append([i, len(word)])\n",
    "        word_counter_.sort(key=lambda x: x[1])\n",
    "        if sort_by == 'largest':\n",
    "            word_counter_.reverse()\n",
    "        word_counter = []\n",
    "        for arr in word_counter_:\n",
    "            word_counter.append(arr[0])\n",
    "\n",
    "        # see which words can be removed (sorted by length)\n",
    "        all_index_good = {}\n",
    "        for i, ind in enumerate(word_counter):\n",
    "            if me.evaluate(s, [ind])[0]:\n",
    "                all_index_good = {ind}\n",
    "                word_counter.pop(i)\n",
    "        if len(all_index_good) == 0:\n",
    "            removable.append(all_index_good)\n",
    "            continue\n",
    "\n",
    "        # see which words can be removed\n",
    "        # just go in order of appearance in sentence\n",
    "        prev_length = -1\n",
    "        curr_length = len(all_index_good)\n",
    "        while prev_length != curr_length:\n",
    "            prev_length = len(all_index_good)\n",
    "            for i, ind in enumerate(word_counter):\n",
    "                if me.evaluate(s, [ind])[0]:\n",
    "                    all_index_good.add(ind)\n",
    "                    word_counter.pop(i)\n",
    "            curr_length = len(all_index_good)\n",
    "        removable.append(all_index_good)\n",
    "    return removable\n",
    "\n",
    "\n",
    "def get_by_impact(me, sort_by='largest', indexes=None):\n",
    "    if sort_by not in ['largest', 'smallest']:\n",
    "        raise ValueError(\"[sort_by] must be largest or smallest\")\n",
    "    removable = []\n",
    "    for s, sentence in enumerate(me.sentences):\n",
    "        # find single indexes that can be removed\n",
    "        single_index_replaceable = set(get_single_index(me, s, sentence))\n",
    "        for index in single_index_replaceable:\n",
    "            if len(sentence[index]) == 1:\n",
    "                single_index_replaceable = single_index_replaceable - {len(sentence)-1}\n",
    "        value = me.evaluate(s, single_index_replaceable)\n",
    "        if len(single_index_replaceable) == 0:\n",
    "            removable.append({})\n",
    "        if value[0]:\n",
    "            removable.append(single_index_replaceable)\n",
    "            continue\n",
    "        invalid_counter_ = []\n",
    "        \n",
    "        # sort words by impact\n",
    "        for single_index in single_index_replaceable:\n",
    "            test_indexes = single_index_replaceable - {single_index}\n",
    "            num_invalid = len(me.evaluate(s, test_indexes)[1])\n",
    "            invalid_counter_.append([single_index, num_invalid])\n",
    "        invalid_counter_.sort(key=lambda x: x[1])\n",
    "        if sort_by == 'largest':\n",
    "            invalid_counter_.reverse()\n",
    "        invalid_counter = []\n",
    "        for arr in invalid_counter_:\n",
    "            invalid_counter.append(arr[0])\n",
    "        all_index_good = {invalid_counter.pop(0)}\n",
    "        iter_max = int(np.log(len(invalid_counter))+1)\n",
    "        iter = 0\n",
    "        \n",
    "        # from the indexes that can be individually removed, try seeing which groups can be removed\n",
    "        all_index_good = __get_by_impact_helper(me, s, all_index_good, invalid_counter, iter_max, iter)\n",
    "\n",
    "        # from the indexes that cannot be individually removed, try adding to the above group\n",
    "        all_index_bad = list(set(np.arange(len(sentence))) - all_index_good - set(invalid_counter))\n",
    "        all_index_good_ = __get_by_impact_helper(me, s, all_index_good, all_index_bad, iter_max, iter)\n",
    "        if len(all_index_good_) > len(all_index_good):\n",
    "            all_index_good.update(all_index_good_)\n",
    "            all_index_good = __get_by_impact_helper(me, s, all_index_good, invalid_counter, iter_max, iter)\n",
    "        removable.append(all_index_good)\n",
    "    return removable\n",
    "\n",
    "\n",
    "def __get_by_impact_helper(me, s, all_index_good, invalid_counter, iter_max, iter):\n",
    "    if iter >= iter_max:\n",
    "        return all_index_good\n",
    "    iter += 1\n",
    "    remove_array = []\n",
    "    for i, index in enumerate(invalid_counter):\n",
    "        temp_all_index_good = all_index_good.union({index})\n",
    "        if me.evaluate(s, temp_all_index_good)[0]:\n",
    "            all_index_good = temp_all_index_good\n",
    "            remove_array.append(i)\n",
    "    for i in sorted(remove_array, reverse=True):\n",
    "        del invalid_counter[i]\n",
    "    if len(remove_array) > 0:\n",
    "        all_index_good.update(__get_by_impact_helper(me, s, all_index_good, invalid_counter, iter_max, iter))\n",
    "    return all_index_good\n",
    "\n",
    "\n",
    "def measure_by_words_removed(token_paragraph, count):\n",
    "    alpha = 0\n",
    "    for myset in token_paragraph:\n",
    "        alpha += len(myset)\n",
    "    print(alpha/count)\n",
    "    return alpha/count\n",
    "\n",
    "def mask_sentences(sentences, removables):\n",
    "    tag_holder = '^'\n",
    "    sentences_ = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        removable = removables[i]\n",
    "        for j, word in enumerate(sentence):\n",
    "            if j in removable:\n",
    "                if word[:2] == '##':\n",
    "                    sentences_[-1] += tag_holder\n",
    "                else:\n",
    "                    sentences_.append(tag_holder)\n",
    "            else:\n",
    "                if word[:2] == '##':\n",
    "                    sentences_[-1] += word[2:]\n",
    "                else:\n",
    "                    sentences_.append(word)\n",
    "    return sentences_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGklCqjyeRhE",
    "colab_type": "text"
   },
   "source": [
    "The dataset to download the data is done here. Wikipedia pages are of variable length. We choose to  stop downloading  wikipedia pages when the total amount of words is greater than 10,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "fJYPZt1HNY2N",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def get_wiki_articles(length=20):\n",
    "    word_arr_all = []\n",
    "    error_counter = 0\n",
    "    while len(word_arr_all) < length:\n",
    "        try:\n",
    "            content = wikipedia.page(wikipedia.random()).content.replace('\\n', ' ')\n",
    "            content_split = content.split(' ')\n",
    "            if content_split[-1][-1] != '.':\n",
    "                content_split[-1] = content_split[-1] + '.'\n",
    "            word_arr_all += content.split(' ')\n",
    "        except:\n",
    "            continue\n",
    "    word_arr = word_arr_all[:length]\n",
    "    if word_arr[-1] != '.':\n",
    "        word_arr.append('.')\n",
    "    return word_arr\n",
    "        \n",
    "def save_text_files(word_arr, folder='.'):\n",
    "    # save the raw text file\n",
    "    with open(folder+'/wiki.txt', 'w') as output:\n",
    "        output.write(' '.join(word_arr))\n",
    "\n",
    "    # do HuffmanCoding on the raw text, and save it\n",
    "    h = HuffmanCoding('')\n",
    "    h.compress_text(word_arr, folder+'/wiki_org.bin')\n",
    "\n",
    "def save_compress_files(me, folder='.'):\n",
    "    length_smallest = get_by_length(me, sort_by='smallest')\n",
    "    length_smallest_sents = mask_sentences(deepcopy(me.sentences), length_smallest)\n",
    "    with open(folder+'/wiki_length_small.txt', 'w') as output:\n",
    "        output.write(' '.join(length_smallest_sents))    \n",
    "    h = HuffmanCoding('')\n",
    "    h.compress_text(length_smallest_sents, folder+'/wiki_length_small.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IqhXlxHmNkhH",
    "colab_type": "code",
    "outputId": "1e3b985d-eed8-46ee-9865-af29eea19647",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1008.0
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 213450/213450 [00:00<00:00, 1058236.22B/s]\n",
      "100%|██████████| 404400730/404400730 [00:10<00:00, 37246534.90B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5\n",
      "0.0\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.6/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir('results')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "# length=1000\n",
    "length=10\n",
    "for i in range(5):\n",
    "    word_arr = get_wiki_articles(length=length)\n",
    "    folder = 'results/{i}'.format(i=i)\n",
    "    try:\n",
    "        os.mkdir(folder)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "        # Tokenize the sentences\n",
    "    word_str = ' '.join(word_arr)  #.replace('\\n', ' ')\n",
    "    me = MaskEvaluator(word_str)\n",
    "\n",
    "    save_text_files(word_arr, folder=folder)\n",
    "    save_compress_files(me, folder=folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "QE6Y-GxkBK-y",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QcoWk-w8NufC",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544.0
    },
    "outputId": "3ef5f6d6-829c-4911-be35-facff3aa18e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: results/ (stored 0%)\n",
      "  adding: results/3/ (stored 0%)\n",
      "  adding: results/3/wiki_length_small.bin (stored 0%)\n",
      "  adding: results/3/wiki_length_small.txt (deflated 12%)\n",
      "  adding: results/3/wiki_org.bin (stored 0%)\n",
      "  adding: results/3/wiki.txt (stored 0%)\n",
      "  adding: results/0/ (stored 0%)\n",
      "  adding: results/0/wiki_length_small.bin (stored 0%)\n",
      "  adding: results/0/wiki_length_small.txt (deflated 23%)\n",
      "  adding: results/0/wiki_org.bin (stored 0%)\n",
      "  adding: results/0/wiki.txt (deflated 8%)\n",
      "  adding: results/2/ (stored 0%)\n",
      "  adding: results/2/wiki_length_small.bin (stored 0%)\n",
      "  adding: results/2/wiki_length_small.txt (deflated 19%)\n",
      "  adding: results/2/wiki_org.bin (stored 0%)\n",
      "  adding: results/2/wiki.txt (deflated 8%)\n",
      "  adding: results/1/ (stored 0%)\n",
      "  adding: results/1/wiki_length_small.bin (stored 0%)\n",
      "  adding: results/1/wiki_length_small.txt (deflated 10%)\n",
      "  adding: results/1/wiki_org.bin (stored 0%)\n",
      "  adding: results/1/wiki.txt (deflated 7%)\n",
      "  adding: results/4/ (stored 0%)\n",
      "  adding: results/4/wiki_length_small.bin (stored 0%)\n",
      "  adding: results/4/wiki_length_small.txt (deflated 7%)\n",
      "  adding: results/4/wiki_org.bin (stored 0%)\n",
      "  adding: results/4/wiki.txt (deflated 6%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r results.zip results/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "project.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
